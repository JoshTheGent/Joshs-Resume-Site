<!DOCTYPE HTML>
<!--
	Industrious by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
		<title>Term Project Page - Joshua Tran</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="icon" href="images/favicon.ico">
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a class="logo" href="index.html">Joshua Tran</a>
				<nav>
					<a href="#menu">Menu</a>
				</nav>
			</header>

		<!-- Nav -->
			<nav id="menu">
				<ul class="links">
					<li><a href="index.html">Home</a></li>
					<li><a href="about.html#projects">Projects</a></li>
					<li><a href="about.html">About</a></li>
				</ul>
			</nav>

		<!-- Heading -->
			<div id="heading" >
				<h1>TERM PROJECT</h1>
			</div>

		<!-- Main -->
			<section id="main" class="wrapper">
				<div class="inner">
					

					<div id="term_proj" class="content">
						<header>
							<h2>Term project</h2>
						</header>
						<hr />
						<h3>Demo</h3>
                        <p>Enter in a review comment, and click Generate Rating. A classifier should then predict the rating based off the given comment.</p>
						

						<form id="reviewForm">
                            <label for="review">Review:</label>
                            <textarea type="text" id="review" name="review"></textarea><br>
                            <a id="fullSend" class="button primary">Generate Rating</a>
                        </form> 
                        
                        <strong>Classified Rating:</strong>
                        <div id="result">
                            Need Review Data
                        </div>
						
                        <hr />
						
                        <h3>Video</h3>
						
						<hr />

                        <h3>Blog</h3>
						<strong>Objective:</strong>
						<p>
							
							The objective of this project is to produce a classifier that can take a look at a given review and produce a predicted rating. These reviews will be based off the datasets defined in the references for the 15 million reviews. Based off these criteria, I'll be implementing a NaiveBayes classifier along side with some pre-built sentiment analyzers to predict the rating of these reviews. However to accomplish this, a few things need to be noted about what has been done.
						</p>

						<strong>Pre-processing the data:</strong>
                        <p>
						The data is vast and large, some of the data needs to pre-processed to ease training, and to get rid of non-helpful information. From the given set of reviews, to simplifiy processing we've made all the classifications into integers 0-10 for 11 total classifications. keeping the floating point representations left review classification values in the thousands of total classifications and seemed over complicated and undesirable. So this has been cleaned up.
						Additionally, I rid the data set from reviews that have no comment, because no comments doesn't help us predict actual comments.
						</p>
						<strong> Methods used: </strong>
						<ul>
							<li>Splitting Data
								<p>
									We need to seperate our data into a Training set and a Testing set. 
									For all methods below we generate a training set of 80% of the reviews and 20% of the reviews.
								</p>
							</li>
							<li>Vader Sentiment
								<p>
									First we have "Vader" sentiment. This will take a given string and find the polarity of how positive, negative, and neutrual it is. 
									We initiallize an analyzer, and give the first comment in our data to it and look at the scores. 
									The idea here is to use the values generated by our analyzer to produce numerical values to use that will weight a Naive Bayes classifier to produce predictions on what the actual weight of the sentiment is.
									It is important to note that this processing takes a little bit of time, so i've generated a routine to check if the the processed sentiment has already been done, and if it has then we can just pull that data.
								</p>
							</li>
							<li>Naive Bayes (W/ hyper parametertuning)
								<p>
									Here we use our Naive bayes classifier mixed with the values used and found in the Vader Sentiment, along with a variety of different smoothing paramters to adjust the alpha value (lap-lace smoothing value) for our classifier. 
									However as the data shows, the hyper parameter applied has no effect on this dataset. and seems to maintain it's accuracy across all instances.
								</p>
							</li>
							<li>TextBlob
								<p>
									Here we try to do something similar to our previous approach. 
									However, lets try to use TextBlob instead. Interestingly enough, TextBlob actually gives us the ability to use a custom classifer to predict what our sentiment is. 
									Rather than just exclusively looking at positive and negative values.
									HOWEVER, this training and classification process is EXTREMELY SLOW. 
									So just to take a look at this, we'll be minimizing the data to around 10 thousand data points that will be split similarly to how we did previously in distribution for test and training sets.
									The way this analyzer works is that it uses a feature extractor to send our given sets in and then apply a classifier(NaiveBayes) on it. 
									However, for it to start producing prediction values it needs an initial comment/classification.
									Additionally it uses an nltk classifier and wraps around it, making it slower than the sci-kit classifiers.
								</p>
							</li>
						</ul> 
						<strong> Conclusion, putting it all together, and Deployment </strong>
                        <p>
							After testing out each method used here. 
							I found that Sci-kit's resources are incredibly fast and optimized, so In practice I'd like to use an approach with it. 
							Additionally we found that smoothing these didn't have too much of an effect on the overall data.
							From Vader, I found that it actually might be best to just transition to losing the sentiment values from training and just use each word as a set of features for our training, as seen in TextBlob due to the increase of accruacy in results. 
							However, TextBlob's problem with processing large amounts of data comes from its feature extraction method. So to speed this up we'll use essentially the same method it uses but port it to sci-kit.
							Here we break our data as usual into train and test. Then create a pipeline with a vectorized feature extractor for each review, then train it on the NaiveBayes classifier.
						</p>
						<p>
							After taking a look at the mixed model produced at the end, we have something that is relatively accurate(over 90% within 3 stars), with decent precision(correct 30%). 
							So i've packaged it up using pickle and deployed it on a pythonanywhere instance. This is used here in the demo above, and the github source link and instruction can be found below.
						</p>
                        <hr />

						<h3>Journal</h3>
						Site Link to Journal: <a href="Term_project.html" ><span class="label">Link to my Site Journal (better viewability)</span></a></br>
						Kaggle link to Journal: <a href="Term_project.html" ><span class="label">Link to my Kaggle Journal(does not support vader sentiment analysis)</span></a></br>
            
						<hr />

                        <h3>Deployment Instructions</h3>
						Github link:
						<a href="https://github.com/j0sh7ran/Data-Mining-Term-Proj" ><span class="label">Link to my implementation</span></a>
						
						<hr />
					</div>
				</div>
			</section>

		<!-- Footer -->
			<footer id="footer">
				<div class="inner">
					<div class="content">
					
						<section >
							<h4>Get ahold of me, <br/> Or check me out!</h4>
							<ul class="plain">
								<li><a ><i class="icon fa-phone">&nbsp;</i>(817)714-2976</a></li>
								<li><a ><i class="icon fa-envelope">&nbsp;</i>j0sh7ran@gmail.com</a></li> 
								<li><a href="https://www.linkedin.com/in/joshua-tran-a5006719a/"><i class="icon fa-linkedin">&nbsp;</i>Linkedin</a></li>
								<li><a href="https://github.com/j0sh7ran"><i class="icon fa-github">&nbsp;</i>Github</a></li>
							</ul>
						</section>

						
					</div>
					<div class="copyright">
						&copy; Joshua Tran. Photos <a href="https://unsplash.co">Unsplash</a>, Video <a href="https://coverr.co">Coverr</a>.
					</div>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
            <script src="assets/js/term.js"></script>
	</body>
</html>